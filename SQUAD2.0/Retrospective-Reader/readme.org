** 论文笔记
   + Retrospective Reader for Machine Reading Comprehension / Rank 2
     + SQUAD2.0数据集包含一部分样本的Q对应的P没有答案存在，因此该模型提出了一个不仅能预测答案在P中的span并且还能预测该样本是否存在答案，这也是真实环境下QA所需要的情景。它指出了目前广泛的QA模型存在的问题——只专注于“阅读”部分来提取答案，而没有关注对答案的验证部分。在模仿人类的阅读理解过程中发现，一般的流程为：1）大体上浏览整篇文章找出问题的大致观点；2）有目的性的阅读文章并且得出确切的答案。实验使用的数据集为SQUAD2.0和NewsQA
     + 模型的处理形式为<P,Q,A>，联合预测答案的存在性以及答案的区间。模型处理流程为粗略阅读模块——>集中阅读模块，粗略阅读模块的作用是判断Q和P是否存在答案，集中阅读模块产生答案并且结合前面的输出来判断最终答案的存在形式。粗略阅读模块的结构类似于Transformer的Encoder部分，使用预训练语言模型的结构，输入到模型的input为word embedding+position embedding+token type embedding，随之输入到encoder模型中获得上下文的表示输出。encoder每一层的模型结构为：[图片插入]输入向量按照多头的数目在最后一个维度拆分为(hiddensize/multi-head)个矩阵，分别经过3个dense层的矩阵变换为K，Q，V矩阵，对于序列中某个token，当计算多头注意力值时，用该token的Q去查询序列中所有token的K，也即是Q乘以每一个K，得到该token对序列长度的注意力权重，然后经过softMax归一化权重，乘以序列长度的V再累加，得到该token对序列长度的注意力值，最后得到每个token对序列的双向自注意力矩阵(seqlen, (hiddensize/multi-head))，随后将多头结果拼接为(seqlen, hiddensize)的矩阵，得到多头自注意力层的输出，随即经过残连接和LayerNorm层，其中LayerNorm对最后一个维度做归一化，得到自注意力层输出。该输出经过feed-forward前馈网络的dense层，激活函数为Gelu，再接一个dense层将hiddensize恢复，随即经过残连接和layernorm层获得这一层encoder layer的输出。特定层数的输入到输出流程的最后得到encoder的输出H={h0,h1...hn